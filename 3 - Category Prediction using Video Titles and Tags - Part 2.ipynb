{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Category Prediction using Video Title and Tags - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Import libraries](#import_libraries)\n",
    "2. [Load GloVe embeddings and processed data](#load)\n",
    "3. [Create GloVe feature vectors](#glove)\n",
    "5. [Create TF-IDF feature vectors](#tfidf)\n",
    "6. [Training and tuning FFN](#ffn) (Commented code contains the combinations we tried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries <a name=\"import_libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AJL\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "cwd = Path('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AJL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AJL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "RANDOM = 42\n",
    "seed(RANDOM)\n",
    "np.random.seed(RANDOM)\n",
    "tf.random.set_seed(RANDOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GloVe embeddings and processed data <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of your glove.twitter.27B.200d.txt file\n",
    "GLOVE_LOCATION = r'D:\\Downloads\\glove.twitter.27B\\glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    words = sentence.split()\n",
    "    avg = np.zeros(200) # depends on glove\n",
    "    num_valid_words = 0\n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            num_valid_words += 1\n",
    "    if num_valid_words == 0:\n",
    "        return np.zeros(200)\n",
    "    else:\n",
    "        return avg / num_valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cwd/'output'/\"title_tags_videos_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GloVe feature vectors <a name=\"glove\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3388/3388 [00:00<00:00, 16654.27it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('X_GloVe.npy'): # if cwd has the file\n",
    "    X = np.load(\"X_GloVe.npy\", allow_pickle=True)\n",
    "else:\n",
    "    title_and_tags = df['title_and_tags'].astype(str).tolist()\n",
    "    word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(GLOVE_LOCATION)\n",
    "    X = np.array([sentence_to_avg(x, word_to_vec_map) for x in tqdm(title_and_tags)])\n",
    "    X = np.nan_to_num(X)\n",
    "    np.save('X_GloVe.npy', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF feature vectors <a name=\"tfidf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df['title_and_tags'].values).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['category_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3388,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3388, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3388, 16376)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Tuning FFN <a name=\"ffn\"></a>\n",
    "Best Combination --- Hidden layers: 768, 384, 64 w/ acc of 0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either use GloVe embeddings or TF-IDF vectors as input. Change network_input according.\n",
    "# If using GLoVe embeddings, you will have 200 dimensions, if TF-IDF then 16376 dimensions.\n",
    "network_input = X_tfidf # or X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3388, 16376)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer_1 = [768] #[1024, 896, 768, 640, 512, 384, 256]\n",
    "num_layer_2 = [384] #[896, 768, 640, 512, 384, 256, 128]\n",
    "num_layer_3 = [64] #[768, 640, 512, 384, 256, 128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations= [(num_1, num_2, num_3) for num_3 in num_layer_3 for num_2 in num_layer_2 for num_1 in num_layer_1\\\n",
    "               if num_1 > num_2 > num_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we use early stopping and restore best wrights, we can afford to set a high epoch - it will automatically stop once loss stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed --------- (768, 384, 64), accuracy: 0.6950146555900574, split: 1/5\n",
      "Completed --------- (768, 384, 64), accuracy: 0.6842877864837646, split: 2/5\n",
      "Completed --------- (768, 384, 64), accuracy: 0.7132353186607361, split: 3/5\n",
      "Completed --------- (768, 384, 64), accuracy: 0.6696296334266663, split: 4/5\n",
      "Completed --------- (768, 384, 64), accuracy: 0.6358209252357483, split: 5/5\n"
     ]
    }
   ],
   "source": [
    "parameters_to_accuracy = {}\n",
    "for permutation in permutations:\n",
    "    accuracies = []\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    skf.get_n_splits(network_input, y)\n",
    "    split_count = 1\n",
    "    for train_index, test_index in skf.split(network_input, y):\n",
    "        #print(f\"Currently doing --- layer_1: {num_1}, layer_2: {num_2}, split: {split_count}/5\")\n",
    "        X_train, X_test = network_input[train_index], network_input[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = Sequential()\n",
    "        for i, layer in enumerate(permutation):\n",
    "            if i == 0:\n",
    "                model.add(Dense(layer, input_dim=network_input.shape[1], activation='relu'))\n",
    "            else:\n",
    "                model.add(Dense(layer, activation='relu'))\n",
    "            model.add(Dropout(rate=0.5))\n",
    "        model.add(Dense(len(Counter(y)), activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        callbacks = [EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True)] #impt\n",
    "        model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks, verbose=0)\n",
    "        _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Completed --------- {permutation}, accuracy: {accuracy}, split: {split_count}/5\")\n",
    "        split_count += 1\n",
    "    average_accuracy = sum(accuracies)/len(accuracies)\n",
    "    parameters_to_accuracy[permutation] = average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorted combinations (best to worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: (768, 384, 64), accuracy: 0.6795976638793946\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(list(parameters_to_accuracy.items()), key=lambda x:x[1], reverse=True):\n",
    "    print(f\"Parameters: {i[0]}, accuracy: {i[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
